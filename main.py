# -*- coding: utf-8 -*-
"""Copy of rp1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13PABWI3Ph5zKt3nMHbGLi4Zo2rpwTnJk
"""

# !wget 'https://nemar.org/dataexplorer/download?filepath=/data/nemar/openneuro//zip_files/ds004504.zip'
# !unzip /content/download?filepath=%2Fdata%2Fnemar%2Fopenneuro%2F%2Fzip_files%2Fds004504.zip

# !pip install python-multipart

# !pip install mne

import mne
import os
os.environ['TF_DISABLE_TFTRT'] = '1'
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import scipy.io
from mne.time_frequency import tfr_array_morlet
from scipy.stats import skew, kurtosis, entropy
import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Softmax, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from sklearn.decomposition import PCA
import sklearn.model_selection
import pickle



# !pip freeze

def prepare_data_training(path):
    raw = mne.io.read_raw_eeglab(path, preload=True)
    tmp = path[23]+path[24]
    person = int(tmp)
    label = 0
    if person <= 36:
      label = 1
    raw.drop_channels(['Fz', 'Pz', 'Cz'])
    #EPOCHING
    epochs = mne.make_fixed_length_epochs(raw, duration=2, overlap = 1, preload=False)
    data = epochs.get_data()
    #FILTERING
    data = mne.filter.filter_data(data, raw.info['sfreq'], 0.5, 45)
    label_array = np.full(data.shape[0], label)
    #SCALING
    scaler_object = mne.decoding.Scaler(info = raw.info)
    scaler_object.fit(data)
    data = scaler_object.transform(data)
    #PCA
    data_2d = np.reshape(data, (data.shape[0]*data.shape[1], -1))
    pca = PCA(n_components = 100)
    data_2d_pca = pca.fit_transform(data_2d)
    data = np.reshape(data_2d_pca, (data.shape[0], data.shape[1], -1))
    data = np.transpose(data, (0, 2, 1))
    return data, label_array

def prepare_data_testing(path):
    raw = mne.io.read_raw_eeglab(path, preload=True)
    raw.drop_channels(['Fz', 'Pz', 'Cz'])
    #EPOCHING
    epochs = mne.make_fixed_length_epochs(raw, duration=2, overlap = 1, preload=False)
    data = epochs.get_data()
    #FILTERING
    data = mne.filter.filter_data(data, raw.info['sfreq'], 0.5, 45)
    #SCALING
    scaler_object = mne.decoding.Scaler(info = raw.info)
    scaler_object.fit(data)
    data = scaler_object.transform(data)
    #PCA
    data_2d = np.reshape(data, (data.shape[0]*data.shape[1], -1))
    pca = PCA(n_components = 100)
    data_2d_pca = pca.fit_transform(data_2d)
    data = np.reshape(data_2d_pca, (data.shape[0], data.shape[1], -1))
    data = np.transpose(data, (0, 2, 1))
    return data

# #DATA PREPARATION
# main_path = "/content/ds004504"
# x_data = np.empty((0, 100, 16))
# y_data = np.empty((0))
# for folder in os.listdir(main_path):
#   if folder[0] == 's' and folder != 'sub-050' and folder != 'sub-051' and folder != 'sub-052' and folder != 'sub-053' and folder != 'sub-054' and folder != 'sub-010' and folder != 'sub-011' and folder != 'sub-012' and folder != 'sub-013' and folder != 'sub-014' and int(folder[5]+folder[6]) <= 65:
#     path = main_path + '/' + folder + '/eeg/' + folder + '_task-eyesclosed_eeg.set'
#     x_data_i, y_data_i = prepare_data_training(path)
#     x_data = np.concatenate((x_data, x_data_i), axis = 0)
#     y_data = np.concatenate((y_data, y_data_i), axis = 0)

# print(x_data.shape)
# print(y_data.shape)

# x_data_train, x_data_val, y_data_train, y_data_val = sklearn.model_selection.train_test_split(x_data, y_data, train_size = 0.60, random_state=42, shuffle=True)

# #MODEL
# model = keras.Sequential()
# model.add(keras.Input(shape = x_data_train[0].shape, name='input'))
# model.add(LSTM(8, return_sequences = True, name = 'lstm1'))
# model.add(Dropout(0.2, name='dropout1'))
# model.add(LSTM(8, name = 'lstm2'))
# model.add(Dropout(0.2, name='dropout2'))
# model.add(Dense(2, name='dense'))
# model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),
#     optimizer = 'adam', metrics = ['accuracy'])
# model.summary()

# history = model.fit(x_data_train, y_data_train, epochs = 5, validation_data = (x_data_val, y_data_val))
# print(history)

with open('finalized_model.sav', 'rb') as model_file:
  model = pickle.load(model_file)

def make_prediction(path_test):
  data_test= prepare_data_testing(path_test)
  y_pred = np.argmax(model.predict(data_test), axis=1)
  mean = np.mean(y_pred)
  if mean < 0.5:
    return "CN"
  else:
    return "AD"



# #TESTING ON 10 SUBJECTS
# predictions = []
# confidence_scores = []
# l = [10, 11, 12, 13, 14, 50, 51, 52, 53, 54]
# for i in l:
#   path_test = '/content/ds004504/sub-0' + str(i) + '/eeg/sub-0' + str(i) + '_task-eyesclosed_eeg.set'
#   data_test = prepare_data_testing(path_test)
#   y_pred = np.argmax(model.predict(data_test), axis=1)
#   mean = np.mean(y_pred)
#   confidence_scores.append(int(100*mean))
#   if mean < 0.5:
#     predictions.append("CN")
#   else:
#     predictions.append("AD")

# print(predictions)
# print(confidence_scores)

# import pickle
# filename = 'finalized_model.sav'
# pickle.dump(model, open(filename, 'wb'))
# loaded_model = pickle.load(open(filename, 'rb'))

# !pip install fastapi
# !pip install uvicorn
# !pip install pickle5
# !pip install pydantic
# !pip install scikit-learn
# !pip install requests
# !pip install pypi-json
# !pip install pyngrok
# !pip install nest-asyncio

# from fastapi import FastAPI
# from pydantic import BaseModel
# import pickle
# import json
# import uvicorn
# from pyngrok import ngrok
# from fastapi.middleware.cors import CORSMiddleware
# import nest_asyncio

# app = FastAPI()

# origins = ["http://localhost:3000"]


# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=origins,
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# !pip install python-multipart

# from fastapi import FastAPI, File, UploadFile
# import os
# from typing import Annotated

# app = FastAPI()

# @app.get('/')
# def index():
#     return {'message': 'This is the homepage of the API shshsh'}

# # Define the directory where uploaded files will be saved
# UPLOAD_DIR = "/content/working/"

# # Create the upload directory if it doesn't exist
# os.makedirs(UPLOAD_DIR, exist_ok=True)

# @app.post("/pred")
# def get_pred(file: UploadFile = File(...)):
#     try:
#         contents = file.file.read()
#         with open(file.filename, 'wb') as f:
#             f.write(contents)
#     except Exception:
#         return {"message": "There was an error uploading the file"}
#     finally:
#         file.file.close()
#     file_path = file.filename
#     pred_name = make_prediction(file_path)
#     return {'prediction': pred_name}

# !ngrok config add-authtoken 2fm7WpiU9aYuOFVcGxtAxakHINr_5mims9zbYPQNmgXHoYH1C

# # ngrok_tunnel = ngrok.connect(8000)
# # print('Public URL: ', ngrok_tunnel.public_url)
# # nest_asyncio.apply()
# # uvicorn.run(app, port=8000)

# port = 8000
# ngrok_tunnel = ngrok.connect(port)

# # where we can visit our fastAPI app
# print('Public URL:', ngrok_tunnel.public_url)
# nest_asyncio.apply()

# # finally run the app
# uvicorn.run(app, port=port)

# # folder_path = '/content/AD/AD1'

# # data_list = []

# # for file_name in os.listdir(folder_path):
# #   if file_name.endswith('.mat'):
# #     file_path = os.path.join(folder_path, file_name)
# #     mat_data = scipy.io.loadmat(file_path)
# #     data_array = mat_data['export']
# #     data_array = np.transpose(data_array)
# #     data_list.append(data_array)

# data = np.concatenate(data_list, axis=1)
# print(data.shape)
# path = '/content/ds004504/sub-001/eeg/sub-001_task-eyesclosed_eeg.set'
# raw = mne.io.read_raw_eeglab(path, preload=True)
# tmp = path[23]+path[24]
# person = int(tmp)
# label = 0
# if person <= 36:
#   label = 1
# raw.drop_channels(['Fz', 'Pz', 'Cz'])
# epochs = mne.make_fixed_length_epochs(raw, duration=2, preload=False)

# data = epochs.get_data()
# label_array = np.full(data.shape[0], label)
# print(data.shape)

# print(raw.info)

# scaler_object = mne.decoding.Scaler(info = raw.info)

# scaler_object.fit(data)

# data = scaler_object.transform(data)

# print(data.shape)

# from sklearn.decomposition import PCA

# data_2d = np.reshape(data, (data.shape[0]*data.shape[1], -1))
# print(data_2d.shape)
# pca = PCA(n_components = 100)
# data_2d_pca = pca.fit_transform(data_2d)
# data = np.reshape(data_2d_pca, (data.shape[0], data.shape[1], -1))
# data = np.transpose(data, (0, 2, 1))
# print(data.shape)